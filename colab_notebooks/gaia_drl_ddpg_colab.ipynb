{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d856229d",
   "metadata": {},
   "source": [
    "# GAIA-DRL: DDPG Training\n",
    "This notebook demonstrates how to train the GAIA-DRL agent using synthetic IoT and geospatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# Carregar amostras de vetor de estado com NDVI (Vt)\n",
    "df = pd.read_csv('../data/Vt_samples.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIAEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(GAIAEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.max_steps = len(df)\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(5,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        return np.array([row['Rt'], row['Et'], row['Lt'], row['It'], row['Vt']], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        reward = 0.25 * row['Et'] + 0.25 * row['Rt'] - 0.2 * row['Lt'] - 0.2 * row['It'] + 0.1 * row['Vt']\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        return self._get_obs(), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAIAEnv(df)\n",
    "obs = env.reset()\n",
    "print('Initial Observation:', obs)\n",
    "\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(f'Action: {action}, Reward: {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c37fdf",
   "metadata": {},
   "source": [
    "## DDPG Implementation\n",
    "Basic implementation of the Deep Deterministic Policy Gradient (DDPG) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt +\n",
    "            self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, 5))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, 5))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.buffer_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf31fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    inputs = layers.Input(shape=(5,))\n",
    "    out = layers.Dense(64, activation='relu')(inputs)\n",
    "    out = layers.Dense(64, activation='relu')(out)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic():\n",
    "    state_input = layers.Input(shape=(5,))\n",
    "    action_input = layers.Input(shape=(1,))\n",
    "    concat = layers.Concatenate()([state_input, action_input])\n",
    "    out = layers.Dense(64, activation='relu')(concat)\n",
    "    out = layers.Dense(64, activation='relu')(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d24589",
   "metadata": {},
   "source": [
    "## Training Loop for GAIA-DRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99296f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e90e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update(state_batch, action_batch, reward_batch, next_state_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        target_actions = target_actor(next_state_batch, training=True)\n",
    "        y = reward_batch + 0.99 * target_critic([next_state_batch, target_actions], training=True)\n",
    "        critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "        critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "    critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "    critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        actions = actor_model(state_batch, training=True)\n",
    "        critic_value = critic_model([state_batch, actions], training=True)\n",
    "        actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "    actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAIAEnv(df)\n",
    "epochs = 50\n",
    "all_rewards = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "    for _ in range(env.max_steps):\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "        action = actor_model(tf_prev_state)\n",
    "        action = action.numpy()[0] + ou_noise()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "        prev_state = state\n",
    "\n",
    "        if buffer.buffer_counter > buffer.batch_size:\n",
    "            states, actions, rewards, next_states = buffer.sample()\n",
    "            update(states, actions, rewards, next_states)\n",
    "\n",
    "    all_rewards.append(episodic_reward)\n",
    "    print(f'Episode {ep+1}, Reward: {episodic_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79edebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_rewards)\n",
    "plt.title('Recompensa total por episódio')\n",
    "plt.xlabel('Episódio')\n",
    "plt.ylabel('Recompensa acumulada')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362c3e6a",
   "metadata": {},
   "source": [
    "## Finalização: Salvando resultados e preparando para análise comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar recompensas em arquivo CSV\n",
    "rewards_df = pd.DataFrame({'episode': list(range(1, len(all_rewards)+1)), 'reward': all_rewards})\n",
    "rewards_df.to_csv('../data/rewards_gaia_drl.csv', index=False)\n",
    "print(\"Recompensas salvas em '../data/rewards_gaia_drl.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sugestão para comparação futura com baseline (exemplo)\n",
    "# baseline_rewards = [100, 110, 95, ...]  # valores fixos simulados\n",
    "# plt.plot(baseline_rewards, label='Baseline')\n",
    "# plt.plot(all_rewards, label='GAIA-DRL')\n",
    "# plt.legend()\n",
    "# plt.title('Comparação entre GAIA-DRL e estratégia estática')\n",
    "# plt.xlabel('Episódio')\n",
    "# plt.ylabel('Recompensa')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
