{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d856229d",
   "metadata": {},
   "source": [
    "# GAIA-DRL: DDPG Training\n",
    "This notebook demonstrates how to train the GAIA-DRL agent using synthetic IoT and geospatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# Carregar amostras de vetor de estado com NDVI (Vt)\n",
    "df = pd.read_csv('../data/Vt_samples.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIAEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(GAIAEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.max_steps = len(df)\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(5,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        return np.array([row['Rt'], row['Et'], row['Lt'], row['It'], row['Vt']], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        reward = 0.25 * row['Et'] + 0.25 * row['Rt'] - 0.2 * row['Lt'] - 0.2 * row['It'] + 0.1 * row['Vt']\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        return self._get_obs(), reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3b82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GAIAEnv(df)\n",
    "obs = env.reset()\n",
    "print('Initial Observation:', obs)\n",
    "\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(f'Action: {action}, Reward: {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c37fdf",
   "metadata": {},
   "source": [
    "## DDPG Implementation\n",
    "Basic implementation of the Deep Deterministic Policy Gradient (DDPG) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt +\n",
    "            self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, 5))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, 5))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.buffer_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf31fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    inputs = layers.Input(shape=(5,))\n",
    "    out = layers.Dense(64, activation='relu')(inputs)\n",
    "    out = layers.Dense(64, activation='relu')(out)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic():\n",
    "    state_input = layers.Input(shape=(5,))\n",
    "    action_input = layers.Input(shape=(1,))\n",
    "    concat = layers.Concatenate()([state_input, action_input])\n",
    "    out = layers.Dense(64, activation='relu')(concat)\n",
    "    out = layers.Dense(64, activation='relu')(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
